{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "COG_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vz-jH8T_Uk2c",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dcGmJaJRnGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhdNGQCmRnGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import reformatInput, load_or_generate_images, iterate_minibatches\n",
        "\n",
        "from model import build_cnn, build_convpool_conv1d, build_convpool_lstm, build_convpool_mix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fsOb9MAyBEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3eOPvxqRnG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timestamp = datetime.datetime.now().strftime('%Y-%m-%d.%H.%M')\n",
        "log_path = os.path.join(\"runs\", timestamp)\n",
        "\n",
        "\n",
        "model_type = '1dconv'      # ['1dconv', 'maxpool', 'lstm', 'mix', 'cnn']\n",
        "log_path = log_path + '_' + model_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a40JieORnHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "dropout_rate = 0.5\n",
        "\n",
        "input_shape = [32, 32, 3]   # 1024\n",
        "nb_class = 4\n",
        "n_colors = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noaCF6mvRnHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# whether to train cnn first, and load its weight for multi-frame model\n",
        "reuse_cnn_flag = False\n",
        "\n",
        "# learning_rate for different models\n",
        "lrs = {\n",
        "    'cnn': 1e-3,\n",
        "    '1dconv': 1e-4,\n",
        "    'lstm': 1e-4,\n",
        "    'mix': 1e-4,\n",
        "}\n",
        "\n",
        "weight_decay = 1e-4\n",
        "learning_rate = lrs[model_type] / 32 * batch_size\n",
        "optimizer = tf.train.AdamOptimizer\n",
        "\n",
        "num_epochs = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUrp6sA_RnHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_decay = 1e-4\n",
        "learning_rate = lrs[model_type] / 32 * batch_size\n",
        "optimizer = tf.train.AdamOptimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqvRWDiZRnHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(images, labels, fold, model_type, batch_size, num_epochs, subj_id=0, reuse_cnn=False, \n",
        "    dropout_rate=dropout_rate ,learning_rate_default=1e-3, Optimizer=tf.train.AdamOptimizer, log_path=log_path):\n",
        "    \"\"\"\n",
        "    A sample training function which loops over the training set and evaluates the network\n",
        "    on the validation set after each epoch. Evaluates the network on the training set\n",
        "    whenever the\n",
        "    :param images: input images\n",
        "    :param labels: target labels\n",
        "    :param fold: tuple of (train, test) index numbers\n",
        "    :param model_type: model type ('cnn', '1dconv', 'lstm', 'mix')\n",
        "    :param batch_size: batch size for training\n",
        "    :param num_epochs: number of epochs of dataset to go over for training\n",
        "    :param subj_id: the id of fold for storing log and the best model\n",
        "    :param reuse_cnn: whether to train cnn first, and load its weight for multi-frame model\n",
        "    :return: none\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.name_scope('Inputs'):\n",
        "        input_var = tf.placeholder(tf.float32, [None, None, 32, 32, n_colors], name='X_inputs')\n",
        "        target_var = tf.placeholder(tf.int64, [None], name='y_inputs')\n",
        "        tf_is_training = tf.placeholder(tf.bool, None, name='is_training')\n",
        "\n",
        "    num_classes = len(np.unique(labels))\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = reformatInput(images, labels, fold)\n",
        "\n",
        "\n",
        "    print('Train set label and proportion:\\t', np.unique(y_train, return_counts=True))\n",
        "    print('Val   set label and proportion:\\t', np.unique(y_val, return_counts=True))\n",
        "    print('Test  set label and proportion:\\t', np.unique(y_test, return_counts=True))\n",
        "\n",
        "    print('The shape of X_trian:\\t', X_train.shape)\n",
        "    print('The shape of X_val:\\t', X_val.shape)\n",
        "    print('The shape of X_test:\\t', X_test.shape)\n",
        "    \n",
        "\n",
        "    print(\"Building model and compiling functions...\")\n",
        "    if model_type == '1dconv':\n",
        "        network = build_convpool_conv1d(input_var, num_classes, train=tf_is_training, \n",
        "                            dropout_rate=dropout_rate, name='CNN_Conv1d'+'_sbj'+str(subj_id))\n",
        "    elif model_type == 'lstm':\n",
        "        network = build_convpool_lstm(input_var, num_classes, 100, train=tf_is_training, \n",
        "                            dropout_rate=dropout_rate, name='CNN_LSTM'+'_sbj'+str(subj_id))\n",
        "    elif model_type == 'mix':\n",
        "        network = build_convpool_mix(input_var, num_classes, 100, train=tf_is_training, \n",
        "                            dropout_rate=dropout_rate, name='CNN_Mix'+'_sbj'+str(subj_id))\n",
        "    elif model_type == 'cnn':\n",
        "        with tf.name_scope(name='CNN_layer'+'_fold'+str(subj_id)):\n",
        "            network = build_cnn(input_var)  # output shape [None, 4, 4, 128]\n",
        "            convpool_flat = tf.reshape(network, [-1, 4*4*128])\n",
        "            h_fc1_drop1 = tf.layers.dropout(convpool_flat, rate=dropout_rate, training=tf_is_training, name='dropout_1')\n",
        "            h_fc1 = tf.layers.dense(h_fc1_drop1, 256, activation=tf.nn.relu, name='fc_relu_256')\n",
        "            h_fc1_drop2 = tf.layers.dropout(h_fc1, rate=dropout_rate, training=tf_is_training, name='dropout_2')\n",
        "            network = tf.layers.dense(h_fc1_drop2, num_classes, name='fc_softmax')\n",
        "            # the loss function contains the softmax activation\n",
        "    else:\n",
        "        raise ValueError(\"Model not supported ['1dconv', 'maxpool', 'lstm', 'mix', 'cnn']\")\n",
        "\n",
        "    Train_vars = tf.trainable_variables()\n",
        "\n",
        "    prediction = network\n",
        "\n",
        "    with tf.name_scope('Loss'):\n",
        "        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in Train_vars if 'kernel' in v.name])\n",
        "        ce_loss = tf.losses.sparse_softmax_cross_entropy(labels=target_var, logits=prediction)\n",
        "        _loss = ce_loss + weight_decay*l2_loss\n",
        "\n",
        "    # decay_steps learning rate decay\n",
        "    decay_steps = 3*(len(y_train)//batch_size)   # len(X_train)//batch_size  the training steps for an epcoh\n",
        "    with tf.name_scope('Optimizer'):\n",
        "        # learning_rate = learning_rate_default * Decay_rate^(global_steps/decay_steps)\n",
        "        global_steps = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        learning_rate = tf.train.exponential_decay(     # learning rate decay\n",
        "            learning_rate_default,  # Base learning rate.\n",
        "            global_steps,\n",
        "            decay_steps,\n",
        "            0.95,  # Decay rate.\n",
        "            staircase=True)\n",
        "        optimizer = Optimizer(learning_rate)    # GradientDescentOptimizer  AdamOptimizer\n",
        "        train_op = optimizer.minimize(_loss, global_step=global_steps, var_list=Train_vars)\n",
        "\n",
        "    with tf.name_scope('Accuracy'):\n",
        "        prediction = tf.argmax(prediction, axis=1)\n",
        "        correct_prediction = tf.equal(prediction, target_var)\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "    # Output directory for models and summaries\n",
        "    # choose different path for different model and subject\n",
        "    out_dir = os.path.abspath(os.path.join(os.path.curdir, log_path, (model_type+'_'+str(subj_id)) ))\n",
        "    print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "    # Summaries for loss, accuracy and learning_rate\n",
        "    loss_summary = tf.summary.scalar('loss', _loss)\n",
        "    acc_summary = tf.summary.scalar('train_acc', accuracy)\n",
        "    lr_summary = tf.summary.scalar('learning_rate', learning_rate)\n",
        "\n",
        "    # Train Summaries\n",
        "    train_summary_op = tf.summary.merge([loss_summary, acc_summary, lr_summary])\n",
        "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, tf.get_default_graph())\n",
        "\n",
        "    # Dev summaries\n",
        "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, tf.get_default_graph())\n",
        "\n",
        "    # Test summaries\n",
        "    test_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "    test_summary_dir = os.path.join(out_dir, \"summaries\", \"test\")\n",
        "    test_summary_writer = tf.summary.FileWriter(test_summary_dir, tf.get_default_graph())\n",
        "\n",
        "\n",
        "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, model_type)\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "\n",
        "    if model_type != 'cnn' and reuse_cnn:\n",
        "        # saver for reuse the CNN weight\n",
        "        reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='VGG_NET_CNN')\n",
        "        original_saver = tf.train.Saver(reuse_vars)         # Pass the variables as a list\n",
        "\n",
        "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    total_start_time = time.time()\n",
        "    best_validation_accu = 0\n",
        "\n",
        "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init_op)\n",
        "        if model_type != 'cnn' and reuse_cnn:\n",
        "            cnn_model_path = os.path.abspath(\n",
        "                                os.path.join(\n",
        "                                    os.path.curdir, log_path, ('cnn_'+str(subj_id)), 'checkpoints' ))\n",
        "            cnn_model_path = tf.train.latest_checkpoint(cnn_model_path)\n",
        "            print('-'*20)\n",
        "            print('Load cnn model weight for multi-frame model from {}'.format(cnn_model_path))\n",
        "            original_saver.restore(sess, cnn_model_path)\n",
        "\n",
        "        stop_count = 0  # count for earlystopping\n",
        "        for epoch in range(num_epochs):\n",
        "            print('-'*50)\n",
        "            # Train set\n",
        "            train_err = train_acc = train_batches = 0\n",
        "            start_time = time.time()\n",
        "            for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=False):\n",
        "                inputs, targets = batch\n",
        "                summary, _, pred, loss, acc = sess.run([train_summary_op, train_op, prediction, _loss, accuracy], \n",
        "                    {input_var: inputs, target_var: targets, tf_is_training: True})\n",
        "                train_acc += acc\n",
        "                train_err += loss\n",
        "                train_batches += 1\n",
        "                train_summary_writer.add_summary(summary, sess.run(global_steps))\n",
        "\n",
        "            av_train_err = train_err / train_batches\n",
        "            av_train_acc = train_acc / train_batches\n",
        "\n",
        "            # Val set\n",
        "            summary, pred, av_val_err, av_val_acc = sess.run([dev_summary_op, prediction, _loss, accuracy],\n",
        "                    {input_var: X_val, target_var: y_val, tf_is_training: False})\n",
        "            dev_summary_writer.add_summary(summary, sess.run(global_steps))\n",
        "\n",
        "            \n",
        "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "                epoch + 1, num_epochs, time.time() - start_time))\n",
        "            \n",
        "            fmt_str = \"Train \\tEpoch [{:d}/{:d}]  train_Loss: {:.4f}\\ttrain_Acc: {:.2f}\"\n",
        "            print_str = fmt_str.format(epoch + 1, num_epochs, av_train_err, av_train_acc*100)\n",
        "            print(print_str)\n",
        "\n",
        "            fmt_str = \"Val \\tEpoch [{:d}/{:d}]  val_Loss: {:.4f}\\tval_Acc: {:.2f}\"\n",
        "            print_str = fmt_str.format(epoch + 1, num_epochs, av_val_err, av_val_acc*100)\n",
        "            print(print_str)\n",
        "            \n",
        "            # Test set\n",
        "            summary, pred, av_test_err, av_test_acc = sess.run([test_summary_op, prediction, _loss, accuracy],\n",
        "                {input_var: X_test, target_var: y_test, tf_is_training: False})\n",
        "            test_summary_writer.add_summary(summary, sess.run(global_steps))\n",
        "            \n",
        "            fmt_str = \"Test \\tEpoch [{:d}/{:d}]  test_Loss: {:.4f}\\ttest_Acc: {:.2f}\"\n",
        "            print_str = fmt_str.format(epoch + 1, num_epochs, av_test_err, av_test_acc*100)\n",
        "            print(print_str)\n",
        "\n",
        "            if av_val_acc > best_validation_accu:   # early_stoping\n",
        "                stop_count = 0\n",
        "                eraly_stoping_epoch = epoch\n",
        "                best_validation_accu = av_val_acc\n",
        "                test_acc_val = av_test_acc\n",
        "                saver.save(sess, checkpoint_prefix, global_step=sess.run(global_steps))\n",
        "            else:\n",
        "                stop_count += 1\n",
        "                if stop_count >= 10: # stop training if val_acc dose not imporve for over 10 epochs\n",
        "                    break\n",
        "\n",
        "        train_batches = train_acc = 0\n",
        "        for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=False):\n",
        "            inputs, targets = batch\n",
        "            acc = sess.run(accuracy, {input_var: X_train, target_var: y_train, tf_is_training: False})\n",
        "            train_acc += acc\n",
        "            train_batches += 1\n",
        "\n",
        "        last_train_acc = train_acc / train_batches\n",
        "        \n",
        "        \n",
        "        last_val_acc = av_val_acc\n",
        "        last_test_acc = av_test_acc\n",
        "        print('-'*50)\n",
        "        print('Time in total:', time.time()-total_start_time)\n",
        "        print(\"Best validation accuracy:\\t\\t{:.2f} %\".format(best_validation_accu * 100))\n",
        "        print(\"Test accuracy when got the best validation accuracy:\\t\\t{:.2f} %\".format(test_acc_val * 100))\n",
        "        print('-'*50)\n",
        "        print(\"Last train accuracy:\\t\\t{:.2f} %\".format(last_train_acc * 100))\n",
        "        print(\"Last validation accuracy:\\t\\t{:.2f} %\".format(last_val_acc * 100))\n",
        "        print(\"Last test accuracy:\\t\\t\\t\\t{:.2f} %\".format(last_test_acc * 100))\n",
        "        print('Early Stopping at epoch: {}'.format(eraly_stoping_epoch+1))\n",
        "\n",
        "    train_summary_writer.close()\n",
        "    dev_summary_writer.close()\n",
        "    test_summary_writer.close()\n",
        "    return [last_train_acc, best_validation_accu, test_acc_val, last_val_acc, last_test_acc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGBLchkoRnH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_all_model(num_epochs=3000):\n",
        "    nums_subject = 13\n",
        "    # Leave-Subject-Out cross validation\n",
        "    subj_nums = np.squeeze(scipy.io.loadmat('/content/sample_data/trials_subNums.mat')['subjectNum'])\n",
        "    fold_pairs = []\n",
        "    for i in np.unique(subj_nums):\n",
        "        ts = subj_nums == i\n",
        "        tr = np.squeeze(np.nonzero(np.bitwise_not(ts)))\n",
        "        ts = np.squeeze(np.nonzero(ts))\n",
        "        np.random.shuffle(tr)\n",
        "        np.random.shuffle(ts)\n",
        "        fold_pairs.append((tr, ts))\n",
        "\n",
        "\n",
        "    images_average, images_timewin, labels = load_or_generate_images(\n",
        "                                                file_path='/content/sample_data/', average_image=3)\n",
        "\n",
        "\n",
        "    print('*'*200)\n",
        "    acc_buf = []\n",
        "    for subj_id in range(nums_subject):\n",
        "        print('-'*100)\n",
        "        \n",
        "        if model_type == 'cnn':\n",
        "            print('The subjects', subj_id, '\\t\\t Training the ' + 'cnn' + ' Model...')\n",
        "            acc_temp = train(images_average, labels, fold_pairs[subj_id], 'cnn', \n",
        "                                batch_size=batch_size, num_epochs=num_epochs, subj_id=subj_id,\n",
        "                                learning_rate_default=lrs['cnn'], Optimizer=optimizer, log_path=log_path)\n",
        "            acc_buf.append(acc_temp)\n",
        "            tf.reset_default_graph()\n",
        "            print('Done!')\n",
        "\n",
        "        else:\n",
        "            # whether to train cnn first, and load its weight for multi-frame model\n",
        "            if reuse_cnn_flag is True:\n",
        "                print('The subjects', subj_id, '\\t\\t Training the ' + 'cnn' + ' Model...')\n",
        "                acc_temp = train(images_average, labels, fold_pairs[subj_id], 'cnn', \n",
        "                                    batch_size=batch_size, num_epochs=num_epochs, subj_id=subj_id,\n",
        "                                    learning_rate_default=lrs['cnn'], Optimizer=optimizer, log_path=log_path)\n",
        "                # acc_buf.append(acc_temp)\n",
        "                tf.reset_default_graph()\n",
        "                print('Done!')\n",
        "        \n",
        "            print('The subjects', subj_id, '\\t\\t Training the ' + model_type + ' Model...')\n",
        "            print('Load the CNN model weight for backbone...')\n",
        "            acc_temp = train(images_timewin, labels, fold_pairs[subj_id], model_type, \n",
        "                            batch_size=batch_size, num_epochs=num_epochs, subj_id=subj_id, reuse_cnn=reuse_cnn_flag, \n",
        "                            learning_rate_default=learning_rate, Optimizer=optimizer, log_path=log_path)\n",
        "                                \n",
        "            acc_buf.append(acc_temp)\n",
        "            tf.reset_default_graph()\n",
        "            print('Done!')\n",
        "        \n",
        "        # return\n",
        "\n",
        "    print('All folds for {} are done!'.format(model_type))\n",
        "    acc_buf = (np.array(acc_buf)).T\n",
        "    acc_mean = np.mean(acc_buf, axis=1).reshape(-1, 1)\n",
        "    acc_buf = np.concatenate([acc_buf, acc_mean], axis=1)\n",
        "    # the last column is the mean of current row\n",
        "    print('Last_train_acc:\\t', acc_buf[0], '\\tmean :', np.mean(acc_buf[0][-1]))\n",
        "    print('Best_val_acc:\\t', acc_buf[1], '\\tmean :', np.mean(acc_buf[1][-1]))\n",
        "    print('Earlystopping_test_acc:\\t', acc_buf[2], '\\tmean :', np.mean(acc_buf[2][-1]))\n",
        "    print('Last_val_acc:\\t', acc_buf[3], '\\tmean :', np.mean(acc_buf[3][-1]))\n",
        "    print('Last_test_acc:\\t', acc_buf[4], '\\tmean :', np.mean(acc_buf[4][-1]))\n",
        "    np.savetxt('./Accuracy_{}.csv'.format(model_type), acc_buf, fmt='%.4f', delimiter=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3fPHFTIRnIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    np.random.seed(2018)\n",
        "    tf.set_random_seed(2018)\n",
        "\n",
        "    train_all_model(num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}